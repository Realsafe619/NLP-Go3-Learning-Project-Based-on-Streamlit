# pages/2_HW02_Representation_Learning.py

import streamlit as st
import pandas as pd
import numpy as np
import re
import jieba
import string
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from gensim.models import Word2Vec
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
import plotly.express as px
import plotly.graph_objects as go
import os
import pickle # ç”¨äºåºåˆ—åŒ–æ¨¡å‹æˆ–æ•°æ®
import json # ç”¨äºåºåˆ—åŒ–é…ç½®

# --- æƒé™æ£€æŸ¥ ---
from utils.user_auth import is_authenticated, get_current_username
if not is_authenticated():
    st.error("âš ï¸ è¯·å…ˆç™»å½•ä»¥è®¿é—®æ­¤åŠŸèƒ½ã€‚")
    st.stop()

# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="HW02: è¡¨å¾å­¦ä¹ ",
    page_icon="ğŸ“š",
    layout="wide"
)

# --- é¡µé¢æ ‡é¢˜ ---
st.title("ğŸ“š HW02: è¡¨å¾å­¦ä¹  (TF-IDF & Word2Vec)")

# --- åˆå§‹åŒ– Session State ---
# ç”¨äºå­˜å‚¨é¢„å¤„ç†åçš„æ•°æ®
if 'df_preprocessed_hw02' not in st.session_state:
    st.session_state.df_preprocessed_hw02 = None
# ç”¨äºå­˜å‚¨ TF-IDF æ¨¡å‹å’Œç»“æœ
if 'tfidf_vectorizer_hw02' not in st.session_state:
    st.session_state.tfidf_vectorizer_hw02 = None
if 'df_tfidf_hw02' not in st.session_state:
    st.session_state.df_tfidf_hw02 = None
# ç”¨äºå­˜å‚¨ Word2Vec æ¨¡å‹å’Œç»“æœ
if 'word2vec_model_hw02' not in st.session_state:
    st.session_state.word2vec_model_hw02 = None
if 'df_weighted_avg_hw02' not in st.session_state:
    st.session_state.df_weighted_avg_hw02 = None

# --- ç”¨æˆ·æ•°æ®è·¯å¾„ ---
current_user = get_current_username()
USER_DATA_DIR = "user_data"
user_models_dir = os.path.join(USER_DATA_DIR, current_user, "models")
user_results_dir = os.path.join(USER_DATA_DIR, current_user, "results")
os.makedirs(user_models_dir, exist_ok=True)
os.makedirs(user_results_dir, exist_ok=True)

# --- åŠŸèƒ½å‡½æ•°å®šä¹‰ ---

def load_stopwords(filepath):
    """åŠ è½½åœç”¨è¯"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return set(line.strip() for line in f)
    except Exception as e:
        st.error(f"åŠ è½½åœç”¨è¯å¤±è´¥ï¼š{e}")
        return set()

def preprocess_text(text, stopwords, remove_punctuation=True, remove_numbers=True, remove_english=True, min_word_len=1):
    """é¢„å¤„ç†å•ä¸ªæ–‡æœ¬"""
    if not isinstance(text, str):
        return []
    text = text.lower()
    if remove_punctuation:
        punctuation = string.punctuation + "ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šâ€œâ€â€˜â€™ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€Â·â€¦â€”"
        text = re.sub(f"[{re.escape(punctuation)}]", "", text)
    if remove_numbers:
        text = re.sub(r'\d+', '', text)
    if remove_english:
        text = re.sub(r'[a-zA-Z]+', '', text) # å»é™¤è‹±æ–‡å•è¯
    words = jieba.lcut(text)
    filtered_words = [word.strip() for word in words if word.strip() not in stopwords and len(word.strip()) >= min_word_len]
    return filtered_words

# --- Tab å¸ƒå±€ ---
tab1, tab2, tab3 = st.tabs(["1ï¸âƒ£ æ•°æ®åŠ è½½ä¸é¢„å¤„ç†", "2ï¸âƒ£ TF-IDF å®éªŒ", "3ï¸âƒ£ Word2Vec å®éªŒ"])

# ==================== Tab 1: æ•°æ®åŠ è½½ä¸é¢„å¤„ç† ====================
with tab1:
    st.header("æ•°æ®åŠ è½½ä¸é¢„å¤„ç†")
    st.write("ä¸Šä¼ CSVæ–‡ä»¶æˆ–ä½¿ç”¨é»˜è®¤æ•°æ®é›†ï¼Œå¹¶è¿›è¡Œæ–‡æœ¬é¢„å¤„ç†ã€‚")

    # ä¸Šä¼ æ–‡ä»¶ or ä½¿ç”¨é»˜è®¤
    uploaded_file = st.file_uploader("ä¸Šä¼  CSV æ–‡ä»¶ï¼ˆè¦æ±‚åŒ…å« 'review' åˆ—ï¼‰", type=["csv"], key="hw02_upload")
    use_default = st.checkbox("ä½¿ç”¨é»˜è®¤æ•°æ®é›†", value=True, key="hw02_default")

    df = None
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file, encoding='utf-8')
        st.success("ä¸Šä¼ æˆåŠŸï¼")
    elif use_default:
        default_dataset_path = r"C:\Users\Railg\Desktop\nlp_go3_project\dataset.csv" # ä½ çš„é»˜è®¤æ•°æ®é›†è·¯å¾„
        try:
            df = pd.read_csv(default_dataset_path, encoding='utf-8')
            st.info(f"ä½¿ç”¨é»˜è®¤æ•°æ®é›†: {default_dataset_path}")
        except FileNotFoundError:
            st.error(f"æ— æ³•æ‰¾åˆ°é»˜è®¤æ•°æ®é›†æ–‡ä»¶: {default_dataset_path}")
            df = None # ç¡®ä¿ df ä¸º Noneï¼Œä»¥ä¾¿åç»­ä¸æ‰§è¡Œé¢„å¤„ç†
        except Exception as e:
            st.error(f"åŠ è½½é»˜è®¤æ•°æ®é›†æ—¶å‡ºé”™: {e}")
            df = None
    else:
        st.warning("è¯·ä¸Šä¼ æ–‡ä»¶æˆ–å‹¾é€‰ä½¿ç”¨é»˜è®¤æ•°æ®é›†ã€‚")

    if df is not None:
        st.subheader("åŸå§‹æ•°æ®é¢„è§ˆ")
        st.dataframe(df.head())

        # é¢„å¤„ç†å‚æ•°è®¾ç½® (æ›´ç¬¦åˆè®¾è®¡æ€è·¯)
        st.subheader("é¢„å¤„ç†å‚æ•°è®¾ç½®")
        stopwords_path = st.text_input("åœç”¨è¯æ–‡ä»¶è·¯å¾„ (å¦‚ä¸ºç©ºåˆ™ä¸ä½¿ç”¨)", value=r"C:\Users\Railg\Desktop\nlp_go3_project\stopwords.txt", key="hw02_stopwords_path") # è®¾ç½®é»˜è®¤è·¯å¾„        
        remove_punctuation = st.toggle("å»æ‰æ ‡ç‚¹", value=True, key="hw02_punct")
        remove_numbers = st.toggle("å»é™¤æ•°å­—", value=True, key="hw02_nums")
        remove_english = st.toggle("å»é™¤è‹±æ–‡", value=True, key="hw02_eng")
        min_word_len = st.slider("æœ€å°è¯é•¿åº¦", min_value=1, max_value=5, value=1, key="hw02_min_len")

        # åŠ è½½åœç”¨è¯
        stopwords = set()
        if stopwords_path and os.path.exists(stopwords_path):
            stopwords = load_stopwords(stopwords_path)
        elif stopwords_path: # å¦‚æœè·¯å¾„ä¸ä¸ºç©ºä½†æ–‡ä»¶ä¸å­˜åœ¨
            st.error(f"åœç”¨è¯æ–‡ä»¶ä¸å­˜åœ¨: {stopwords_path}")

        # æ‰§è¡Œé¢„å¤„ç†
        if st.button("æ‰§è¡Œé¢„å¤„ç†", key="hw02_preprocess_btn"):
            # è·å–å½“å‰ç»„ä»¶çš„å€¼ï¼Œç¡®ä¿å®ƒä»¬æ˜¯æœ€æ–°çš„
            processed_stopwords_path = st.session_state["hw02_stopwords_path"]
            processed_remove_punctuation = st.session_state["hw02_punct"]
            processed_remove_numbers = st.session_state["hw02_nums"]
            processed_remove_english = st.session_state["hw02_eng"]
            processed_min_word_len = st.session_state["hw02_min_len"]
            
            # é‡æ–°åŠ è½½åœç”¨è¯ï¼ˆä»¥é˜²è·¯å¾„æ”¹å˜ï¼‰
            processed_stopwords = set()
            if processed_stopwords_path and os.path.exists(processed_stopwords_path):
                processed_stopwords = load_stopwords(processed_stopwords_path)
            
            with st.spinner("æ­£åœ¨é¢„å¤„ç†æ•°æ®..."):
                df['review_wd'] = df['review'].apply(
                    lambda x: preprocess_text(
                        x, 
                        processed_stopwords, 
                        processed_remove_punctuation, 
                        processed_remove_numbers, 
                        processed_remove_english, 
                        processed_min_word_len
                    )
                )
            st.success("é¢„å¤„ç†å®Œæˆï¼")
            st.subheader("é¢„å¤„ç†åæ•°æ®é¢„è§ˆ")
            # æ˜¾ç¤ºåŸå¥ vs é¢„å¤„ç†ç»“æœ (ä¸¤åˆ—å¯¹ç…§)
            preview_df = df[['review', 'review_wd']].head(10).copy()
            preview_df['review_wd_str'] = preview_df['review_wd'].apply(lambda x: ', '.join(x))
            st.dataframe(preview_df[['review', 'review_wd_str']].rename(columns={'review_wd_str': 'é¢„å¤„ç†ç»“æœ'}))

            # ç»Ÿè®¡ä¿¡æ¯
            st.subheader("é¢„å¤„ç†ç»Ÿè®¡ä¿¡æ¯")
            df['word_count'] = df['review_wd'].apply(len)
            avg_len = df['word_count'].mean()
            max_len = df['word_count'].max()
            min_len = df['word_count'].min()
            st.write(f"- å¹³å‡è¯æ•°: {avg_len:.2f}")
            st.write(f"- æœ€é•¿è¯„è®ºè¯æ•°: {max_len}")
            st.write(f"- æœ€çŸ­è¯„è®ºè¯æ•°: {min_len}")

            # ä¿å­˜åˆ° session_state
            st.session_state.df_preprocessed_hw02 = df.copy()
            st.session_state.hw02_preprocess_params = {
                "stopwords_path": processed_stopwords_path,
                "remove_punctuation": processed_remove_punctuation,
                "remove_numbers": processed_remove_numbers,
                "remove_english": processed_remove_english,
                "min_word_len": processed_min_word_len
            }
            st.success("é¢„å¤„ç†æ•°æ®å·²ä¿å­˜è‡³ä¼šè¯çŠ¶æ€ã€‚")


# ==================== Tab 2: TF-IDF å®éªŒ ====================
with tab2:
    st.header("TF-IDF å®éªŒ")
    st.write("è®¡ç®—TF-IDFæƒé‡ï¼Œæå–å…³é”®è¯ï¼Œå¹¶ç”Ÿæˆè¯äº‘ã€‚")

    # æ£€æŸ¥æ˜¯å¦æœ‰é¢„å¤„ç†æ•°æ®
    if st.session_state.df_preprocessed_hw02 is None:
        st.warning("è¯·å…ˆåœ¨ 'æ•°æ®åŠ è½½ä¸é¢„å¤„ç†' é¡µé¢å®Œæˆæ•°æ®é¢„å¤„ç†ã€‚")
    else:
        df = st.session_state.df_preprocessed_hw02

        # å‚æ•°è®¾ç½® (æ›´ç¬¦åˆè®¾è®¡æ€è·¯)
        st.subheader("TF-IDF å…³é”®è¯æå–æ¨¡å—")
        top_k = st.slider("topK å…³é”®è¯æ•°é‡", min_value=5, max_value=50, value=20, key="hw02_topk")
        ngram_range = st.selectbox("ngram èŒƒå›´", options=[(1,1), (1,2), (1,3)], index=1, key="hw02_ngram")
        min_df = st.slider("æœ€å°æ–‡æ¡£é¢‘ç‡ (min_df)", min_value=1, max_value=10, value=1, key="hw02_min_df")
        max_features = st.slider("æœ€å¤§ç‰¹å¾æ•° (max_features)", min_value=100, max_value=5000, value=1000, key="hw02_max_feat")

        # å‡†å¤‡è¯­æ–™
        df['review_for_tfidf'] = df['review_wd'].apply(lambda x: ' '.join(x))

        # æ‰§è¡Œ TF-IDF
        if st.button("æ‰§è¡Œ TF-IDF å…³é”®è¯æå–", key="hw02_tfidf_btn"):
            with st.spinner("æ­£åœ¨è®¡ç®— TF-IDF..."):
                vectorizer = TfidfVectorizer(
                    stop_words=None, # åœç”¨è¯å·²åœ¨é¢„å¤„ç†ä¸­å¤„ç†
                    ngram_range=ngram_range,
                    min_df=min_df,
                    max_features=max_features
                )
                tfidf_matrix = vectorizer.fit_transform(df['review_for_tfidf'].tolist())
                feature_names = vectorizer.get_feature_names_out()

                # æå–å…³é”®è¯
                top_keywords_list = []
                for i in range(tfidf_matrix.shape[0]):
                    tfidf_scores = tfidf_matrix[i].toarray().flatten()
                    sorted_indices = tfidf_scores.argsort()[::-1][:top_k]
                    top_keywords = [feature_names[idx] for idx in sorted_indices if tfidf_scores[idx] > 0]
                    top_keywords_list.append(top_keywords)

                df['tfidf_keywords'] = top_keywords_list

            st.success("TF-IDF å…³é”®è¯æå–å®Œæˆï¼")
            st.session_state.df_tfidf_hw02 = df.copy()
            st.session_state.tfidf_vectorizer_hw02 = vectorizer

            # æ˜¾ç¤ºç»“æœ (DataFrame)
            st.subheader("æ¯æ¡è¯„è®ºçš„ Top-K å…³é”®è¯")
            display_df = df[['review', 'tfidf_keywords']].copy()
            display_df['tfidf_keywords_str'] = display_df['tfidf_keywords'].apply(lambda x: ', '.join(x))
            st.dataframe(display_df[['review', 'tfidf_keywords_str']].head(10).rename(columns={'tfidf_keywords_str': 'TF-IDF å…³é”®è¯'}))

            # å•æ¡è¯„è®ºå…³é”®è¯æŸ¥è¯¢
            st.subheader("å•æ¡è¯„è®ºå…³é”®è¯æŸ¥è¯¢")
            def safe_string_slice(s, length=50):
                if isinstance(s, str):
                    return s[:length]
                else:
                    return ""
                    
            selected_index = st.selectbox("é€‰æ‹©è¯„è®º", options=range(len(df)), format_func=lambda x: f"è¯„è®º {x+1}: {safe_string_slice(df.iloc[x]['review'])}...")
            show_keywords = st.button("æŸ¥è¯¢å…³é”®è¯", key="hw02_query_single")
            
            # ä¿å­˜æŸ¥è¯¢ç»“æœçŠ¶æ€
            if 'show_keyword_result' not in st.session_state:
                st.session_state.show_keyword_result = False
            if 'last_selected_index' not in st.session_state:
                st.session_state.last_selected_index = None
                
            if show_keywords:
                st.session_state.show_keyword_result = True
                st.session_state.last_selected_index = selected_index
                
            if st.session_state.show_keyword_result and st.session_state.last_selected_index is not None:
                current_index = st.session_state.last_selected_index
                selected_keywords = df.iloc[current_index]['tfidf_keywords']
                st.write(f"è¯„è®º {current_index+1} çš„å…³é”®è¯: {selected_keywords}")
                # å…³é”®è¯æŸ±çŠ¶å›¾
                if selected_keywords:
                    keyword_scores = [vectorizer.transform([df.iloc[current_index]['review_for_tfidf']]).toarray()[0][vectorizer.vocabulary_[kw]] for kw in selected_keywords if kw in vectorizer.vocabulary_]
                    fig_bar = px.bar(x=selected_keywords, y=keyword_scores, labels={'x': 'å…³é”®è¯', 'y': 'TF-IDF æƒé‡'}, title=f"è¯„è®º {current_index+1} çš„å…³é”®è¯æƒé‡")
                    st.plotly_chart(fig_bar, use_container_width=True)


        # TF-IDF è¯äº‘æ¨¡å—
        if st.session_state.df_tfidf_hw02 is not None and st.session_state.tfidf_vectorizer_hw02 is not None:
            st.subheader("TF-IDF è¯äº‘æ¨¡å—")
            # å‚æ•°è®¾ç½®
            bg_color = st.color_picker("èƒŒæ™¯é¢œè‰²", value="#ffffff", key="hw02_wc_bg")
            max_words = st.slider("æœ€å¤§è¯æ•°", min_value=50, max_value=500, value=200, key="hw02_wc_max")
            # mask_image = st.file_uploader("ä¸Šä¼  Mask å›¾ç‰‡ (å¯é€‰)", type=["png", "jpg", "jpeg"], key="hw02_wc_mask")

            if st.button("ç”Ÿæˆ TF-IDF è¯äº‘", key="hw02_wc_btn"):
                df_tfidf = st.session_state.df_tfidf_hw02
                vectorizer = st.session_state.tfidf_vectorizer_hw02
                # ä½¿ç”¨æ‰€æœ‰æ–‡æ¡£çš„ TF-IDF çŸ©é˜µï¼Œè·å–å¹³å‡æƒé‡æˆ–æ€»æƒé‡æ¥ç”Ÿæˆè¯äº‘
                tfidf_matrix_full = vectorizer.transform(df_tfidf['review_for_tfidf'].tolist())
                # è®¡ç®—æ¯ä¸ªè¯çš„å¹³å‡TF-IDFåˆ†æ•°ä½œä¸ºæƒé‡
                mean_scores = np.array(tfidf_matrix_full.mean(axis=0)).flatten()
                feature_names = vectorizer.get_feature_names_out()
                word_freq_dict = dict(zip(feature_names, mean_scores))

                if word_freq_dict:
                    # ç¡®ä¿å­—ä½“è·¯å¾„æ­£ç¡®ï¼Œè¿™é‡Œä½¿ç”¨ matplotlib é»˜è®¤å­—ä½“æˆ– simhei.ttf
                    # æ³¨æ„ï¼šåœ¨ Streamlit Cloud ç­‰ç¯å¢ƒä¸­ï¼Œå­—ä½“è·¯å¾„å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
                    try:
                        # å°è¯•ä½¿ç”¨ matplotlib å­—ä½“
                        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans', 'Arial Unicode MS'] # è°ƒæ•´å­—ä½“ä¼˜å…ˆçº§
                        plt.rcParams['axes.unicode_minus'] = False
                        
                        # æ£€æŸ¥ç³»ç»Ÿä¸­å¯ç”¨çš„å­—ä½“
                        import matplotlib.font_manager as fm
                        available_fonts = [f.name for f in fm.fontManager.ttflist]
                        
                        # æŸ¥æ‰¾å¯ç”¨çš„ä¸­æ–‡å­—ä½“
                        chinese_font = None
                        preferred_fonts = ['SimHei', 'Microsoft YaHei', 'STHeiti', 'SimSun']
                        for font_name in preferred_fonts:
                            if font_name in available_fonts:
                                chinese_font = font_name
                                break
                        
                        # æ„å»ºWordCloudå‚æ•°
                        wc_params = {
                            "background_color": bg_color,
                            "width": 800,
                            "height": 400,
                            "max_words": max_words,
                            "relative_scaling": 0.5,
                            "colormap": 'viridis',
                            "font_path": None  # é»˜è®¤ä¸æŒ‡å®šå­—ä½“è·¯å¾„
                        }
                        
                        # å¦‚æœæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œåˆ™æ·»åŠ font_pathå‚æ•°
                        if chinese_font:
                            # è·å–å­—ä½“è·¯å¾„
                            font_paths = [f.fname for f in fm.fontManager.ttflist if f.name == chinese_font]
                            if font_paths:
                                wc_params["font_path"] = font_paths[0]
                        
                        wordcloud = WordCloud(**wc_params).generate_from_frequencies(word_freq_dict)

                        fig_wc, ax = plt.subplots(figsize=(15, 7))
                        ax.imshow(wordcloud, interpolation='bilinear')
                        ax.axis('off')
                        ax.set_title('TF-IDF æƒé‡è¯äº‘', fontsize=16)
                        st.pyplot(fig_wc)
                    except Exception as e:
                        st.error(f"ç”Ÿæˆè¯äº‘æ—¶å‡ºé”™ (å¯èƒ½ä¸å­—ä½“æœ‰å…³): {e}")
                        # å°è¯•ä¸æŒ‡å®šå­—ä½“ç”Ÿæˆ
                        try:
                            # ä½¿ç”¨æœ€åŸºæœ¬çš„é…ç½®å°è¯•ç”Ÿæˆè¯äº‘
                            wordcloud = WordCloud(
                                background_color=bg_color,
                                width=800,
                                height=400,
                                max_words=max_words,
                                relative_scaling=0.5,
                                colormap='viridis',
                                # å¼ºåˆ¶ä¸ä½¿ç”¨ç‰¹å®šå­—ä½“è·¯å¾„
                            ).generate_from_frequencies(word_freq_dict)

                            fig_wc2, ax2 = plt.subplots(figsize=(15, 7))
                            ax2.imshow(wordcloud, interpolation='bilinear')
                            ax2.axis('off')
                            ax2.set_title('TF-IDF æƒé‡è¯äº‘ (åŸºç¡€ç‰ˆæœ¬)', fontsize=16)
                            st.pyplot(fig_wc2)
                        except Exception as e2:
                            st.error(f"å°è¯•å¤‡ç”¨æ–¹æ³•ç”Ÿæˆè¯äº‘ä¹Ÿå¤±è´¥: {e2}")

                else:
                    st.warning("æ²¡æœ‰æœ‰æ•ˆçš„TF-IDFæƒé‡ç”¨äºç”Ÿæˆè¯äº‘ã€‚")


# ==================== Tab 3: Word2Vec å®éªŒ ====================
with tab3:
    st.header("Word2Vec å®éªŒ")
    st.write("è®­ç»ƒWord2Vecæ¨¡å‹ï¼ŒæŸ¥è¯¢ç›¸ä¼¼è¯ï¼Œå¹¶è¿›è¡Œè¯å‘é‡å¯è§†åŒ–ã€‚")

    # æ£€æŸ¥æ˜¯å¦æœ‰é¢„å¤„ç†æ•°æ®
    if st.session_state.df_preprocessed_hw02 is None:
        st.warning("è¯·å…ˆåœ¨ 'æ•°æ®åŠ è½½ä¸é¢„å¤„ç†' é¡µé¢å®Œæˆæ•°æ®é¢„å¤„ç†ã€‚")
    else:
        df = st.session_state.df_preprocessed_hw02

        # å‚æ•°è®¾ç½® (æ›´ç¬¦åˆè®¾è®¡æ€è·¯)
        st.subheader("Word2Vec æ¨¡å‹è®­ç»ƒ")
        vector_size = st.slider("å‘é‡ç»´åº¦", min_value=50, max_value=300, value=100, key="hw02_w2v_vs")
        window = st.slider("çª—å£å¤§å°", min_value=2, max_value=10, value=5, key="hw02_w2v_win")
        min_count = st.slider("æœ€å°è¯é¢‘", min_value=1, max_value=10, value=1, key="hw02_w2v_mc")
        sg = st.radio("æ¨¡å‹ç±»å‹", ["CBOW (sg=0)", "Skip-Gram (sg=1)"], index=1, key="hw02_w2v_sg")
        sg_val = 1 if sg == "Skip-Gram (sg=1)" else 0

        # è®­ç»ƒæ¨¡å‹ or åŠ è½½æ¨¡å‹
        model = st.session_state.word2vec_model_hw02
        train_model = st.checkbox("é‡æ–°è®­ç»ƒæ¨¡å‹ï¼ˆè€—æ—¶è¾ƒé•¿ï¼‰", value=True, key="hw02_w2v_train")

        if train_model:
            if st.button("å¼€å§‹è®­ç»ƒ Word2Vec æ¨¡å‹", key="hw02_w2v_train_btn"):
                with st.spinner("æ­£åœ¨è®­ç»ƒ Word2Vec æ¨¡å‹..."):
                    sentences = df['review_wd'].tolist()
                    model = Word2Vec(
                        sentences=sentences,
                        vector_size=vector_size,
                        window=window,
                        min_count=min_count,
                        workers=4,
                        sg=sg_val
                    )
                st.success("æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
                st.session_state.word2vec_model_hw02 = model
                # å¯é€‰ï¼šä¿å­˜æ¨¡å‹åˆ°ç”¨æˆ·ç›®å½•
                model_save_path = os.path.join(user_models_dir, f"hw02_word2vec_user_{current_user}.model")
                model.save(model_save_path)
                st.info(f"æ¨¡å‹å·²ä¿å­˜è‡³: {model_save_path}")
        else:
            # åŠ è½½æ¨¡å‹
            model_path_input = st.text_input("ä»æ–‡ä»¶åŠ è½½æ¨¡å‹è·¯å¾„", value=os.path.join(r"C:\Users\Railg\Desktop\nlp_go3_project", f"hw02_word2vec_user_{current_user}.model"), key="hw02_w2v_load_path") # ä¿®æ”¹é»˜è®¤è·¯å¾„
            if st.button("åŠ è½½æ¨¡å‹", key="hw02_w2v_load_btn"):
                try:
                    model = Word2Vec.load(model_path_input)
                    st.success("æ¨¡å‹åŠ è½½æˆåŠŸï¼")
                    st.session_state.word2vec_model_hw02 = model
                except Exception as e:
                    st.error(f"åŠ è½½æ¨¡å‹å¤±è´¥ï¼š{e}")


        # æ¨¡å‹åŠŸèƒ½å±•ç¤º
        if st.session_state.word2vec_model_hw02 is not None:
            model = st.session_state.word2vec_model_hw02
            st.subheader("æ¨¡å‹ä¿¡æ¯ä¸åŠŸèƒ½")
            st.write(f"- è¯æ±‡è¡¨å¤§å°ï¼š{len(model.wv.key_to_index)}")
            st.write(f"- å‘é‡ç»´åº¦ï¼š{model.vector_size}")

            # ç›¸ä¼¼è¯æŸ¥è¯¢
            st.subheader("ç›¸ä¼¼è¯æŸ¥è¯¢")
            query_word = st.text_input("è¾“å…¥æŸ¥è¯¢è¯", value="é…’åº—", key="hw02_w2v_query")
            top_n_similar = st.slider("æ˜¾ç¤ºæœ€ç›¸ä¼¼è¯æ•°é‡", min_value=1, max_value=20, value=10, key="hw02_w2v_topn")

            if st.button("æŸ¥è¯¢ç›¸ä¼¼è¯", key="hw02_w2v_query_btn"):
                try:
                    similar_words = model.wv.most_similar(query_word, topn=top_n_similar)
                    st.write(f"'{query_word}' çš„æœ€ç›¸ä¼¼è¯:")
                    for word, score in similar_words:
                        st.write(f"- {word}: {score:.4f}")
                except KeyError:
                    st.warning(f"è¯ '{query_word}' ä¸åœ¨è¯æ±‡è¡¨ä¸­ã€‚")


            # PCA é™ç»´ + å‘é‡å¯è§†åŒ–
            st.subheader("è¯å‘é‡å¯è§†åŒ– (PCA)")
            st.write("ä½¿ç”¨PCAå°†è¯å‘é‡é™ç»´è‡³2Dè¿›è¡Œå¯è§†åŒ–ã€‚")

            custom_words = st.text_area("è¾“å…¥è¦å¯è§†åŒ–çš„è¯ï¼ˆç”¨é€—å·æˆ–ç©ºæ ¼åˆ†éš”ï¼‰", "å¹²å‡€,æ•´æ´,èˆ’é€‚,æ¸©é¦¨,å®‰é™,ç®€é™‹,ç ´æ—§,å¹´ä»£,æ¶åŠ£,ç¬‘å®¹,ç»†å¿ƒ", key="hw02_w2v_pca_words")
            word_list = [w.strip() for w in re.split(r'[,\s]+', custom_words) if w.strip()]

            if st.button("ç”Ÿæˆ PCA å›¾", key="hw02_w2v_pca_btn"):
                vectors = []
                valid_words = []
                for word in word_list:
                    try:
                        vectors.append(model.wv[word])
                        valid_words.append(word)
                    except KeyError:
                        st.warning(f"è¯ '{word}' ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œå·²è·³è¿‡ã€‚")
                        continue

                if len(vectors) == 0:
                    st.warning("æ²¡æœ‰æœ‰æ•ˆçš„è¯å‘é‡å¯ä¾›ç»˜åˆ¶ã€‚")
                else:
                    vectors = np.array(vectors)
                    pca = PCA(n_components=2)
                    reduced_vectors = pca.fit_transform(vectors)

                    # ä½¿ç”¨ Plotly åˆ›å»ºäº¤äº’å¼å›¾è¡¨
                    fig_pca = go.Figure()
                    fig_pca.add_trace(go.Scatter(
                        x=reduced_vectors[:, 0],
                        y=reduced_vectors[:, 1],
                        mode='markers+text',
                        text=valid_words,
                        textposition="top center",
                        marker=dict(size=8, opacity=0.7),
                        name="Words"
                    ))
                    fig_pca.update_layout(
                        title='è¯å‘é‡åˆ†å¸ƒå›¾ï¼ˆPCA 2Dï¼‰',
                        xaxis_title='PCA Component 1',
                        yaxis_title='PCA Component 2',
                        width=800,
                        height=600,
                        hovermode='closest'
                    )
                    st.plotly_chart(fig_pca, use_container_width=True)


            # è®¡ç®—åŠ æƒå¹³å‡å‘é‡ (å¦‚æœTF-IDFç»“æœå­˜åœ¨)
            if st.session_state.df_tfidf_hw02 is not None:
                df_tfidf = st.session_state.df_tfidf_hw02
                st.subheader("è¯„è®ºå‘é‡å¯è§†åŒ– (åŸºäºTF-IDFåŠ æƒå¹³å‡)")

                # ä¸ºæ¯è¡Œè®¡ç®—åŠ æƒå¹³å‡å‘é‡
                if st.button("è®¡ç®—TF-IDFåŠ æƒå¹³å‡å‘é‡", key="hw02_w2v_weighted_btn"):
                    vectorizer_full = st.session_state.tfidf_vectorizer_hw02 # ä½¿ç”¨ä¹‹å‰è®­ç»ƒå¥½çš„TF-IDFæ¨¡å‹
                    if vectorizer_full is None:
                        st.error("æ— æ³•è®¡ç®—åŠ æƒå‘é‡ï¼ŒTF-IDFæ¨¡å‹æœªæ‰¾åˆ°ã€‚è¯·å…ˆè¿è¡ŒTF-IDFå®éªŒã€‚")
                    else:
                        with st.spinner("è®¡ç®—åŠ æƒå¹³å‡å‘é‡..."):
                            # é‡æ–°è®¡ç®—TF-IDFçŸ©é˜µï¼ˆç”¨äºè·å–ç²¾ç¡®æƒé‡ï¼‰
                            tfidf_matrix_full = vectorizer_full.fit_transform(df_tfidf['review_for_tfidf'].tolist())
                            feature_names = vectorizer_full.get_feature_names_out()

                            df_tfidf['tfidf_word_weights'] = [
                                dict(zip(feature_names, tfidf_matrix_full[i].toarray().flatten()))
                                for i in range(tfidf_matrix_full.shape[0])
                            ]

                            def get_weighted_average_vector(tfidf_keywords, tfidf_scores_dict, model):
                                vectors = []
                                weights = []
                                for word in tfidf_keywords:
                                    if word in model.wv.key_to_index and word in tfidf_scores_dict:
                                        vectors.append(model.wv[word])
                                        weights.append(tfidf_scores_dict[word])
                                if len(vectors) == 0:
                                    return np.zeros(model.vector_size)
                                vectors = np.array(vectors)
                                weights = np.array(weights)
                                if weights.sum() > 0:
                                    weights = weights / weights.sum()
                                return np.average(vectors, axis=0, weights=weights)

                            df_tfidf['weighted_avg_vec'] = df_tfidf.apply(
                                lambda row: get_weighted_average_vector(row['tfidf_keywords'], row['tfidf_word_weights'], model),
                                axis=1
                            )

                        st.success("åŠ æƒå¹³å‡å‘é‡è®¡ç®—å®Œæˆï¼")
                        st.session_state.df_weighted_avg_hw02 = df_tfidf.copy() # ä¿å­˜åˆ° session_state


                # PCA å¯è§†åŒ–åŠ æƒå¹³å‡å‘é‡ (æŒ‰ç±»åˆ«æŸ“è‰²)
                if st.session_state.df_weighted_avg_hw02 is not None:
                    df_weighted = st.session_state.df_weighted_avg_hw02
                    if 'label' in df_weighted.columns:
                        st.subheader("è¯„è®ºå‘é‡ PCA å¯è§†åŒ–ï¼ˆæŒ‰ç±»åˆ«æŸ“è‰²ï¼‰")
                        n_each = st.slider("æ¯ç±»æ ·æœ¬æ•°é‡", min_value=5, max_value=100, value=20, key="hw02_w2v_samples")

                        if st.button("ç”Ÿæˆ PCA å›¾ï¼ˆæŒ‰ç±»åˆ«ï¼‰", key="hw02_w2v_pca_label_btn"):
                            df_0 = df_weighted[df_weighted['label'] == 0].sample(n=min(n_each, len(df_weighted[df_weighted['label'] == 0])), random_state=42)
                            df_1 = df_weighted[df_weighted['label'] == 1].sample(n=min(n_each, len(df_weighted[df_weighted['label'] == 1])), random_state=42)
                            df_samples = pd.concat([df_0, df_1]).reset_index(drop=True)

                            vector_matrix_weighted = np.vstack(df_samples['weighted_avg_vec'].values)
                            pca_weighted = PCA(n_components=2)
                            reduced_vectors_weighted = pca_weighted.fit_transform(vector_matrix_weighted)

                            fig_pca_label = go.Figure()
                            fig_pca_label.add_trace(go.Scatter(
                                x=reduced_vectors_weighted[:len(df_0), 0],
                                y=reduced_vectors_weighted[:len(df_0), 1],
                                mode='markers',
                                marker=dict(color='lightblue', size=8, line=dict(color='darkblue', width=1)),
                                name='Label 0 (å·®è¯„)',
                                opacity=0.7
                            ))
                            fig_pca_label.add_trace(go.Scatter(
                                x=reduced_vectors_weighted[len(df_0):, 0],
                                y=reduced_vectors_weighted[len(df_0):, 1],
                                mode='markers',
                                marker=dict(color='lightcoral', size=8, line=dict(color='darkred', width=1)),
                                name='Label 1 (å¥½è¯„)',
                                opacity=0.7
                            ))
                            fig_pca_label.update_layout(
                                title='PCA å¯è§†åŒ–ï¼ˆè¯„è®ºTF-IDFåŠ æƒå¹³å‡å‘é‡ï¼ŒæŒ‰æ ‡ç­¾æŸ“è‰²ï¼‰',
                                xaxis_title='PCA Component 1',
                                yaxis_title='PCA Component 2',
                                width=800,
                                height=600,
                                hovermode='closest'
                            )
                            st.plotly_chart(fig_pca_label, use_container_width=True)

                    else:
                        st.warning("æ•°æ®ä¸­æ²¡æœ‰ 'label' åˆ—ï¼Œæ— æ³•æŒ‰æ ‡ç­¾å¯è§†åŒ–è¯„è®ºå‘é‡ã€‚")


# --- ä¿¡æ¯æç¤º ---
st.divider()
st.info("ğŸ’¡ æç¤ºï¼šæ­¤é¡µé¢æ•´åˆäº†HW02çš„å…¨éƒ¨å®éªŒå†…å®¹ã€‚æ•°æ®å’Œæ¨¡å‹å·²ä¸ç”¨æˆ·ä¸­å¿ƒå…³è”ï¼Œè®­ç»ƒç»“æœä¼šè‡ªåŠ¨ä¿å­˜ã€‚")